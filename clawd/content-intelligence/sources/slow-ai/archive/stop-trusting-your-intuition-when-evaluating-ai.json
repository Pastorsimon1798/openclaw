{
  "url": "https://theslowai.substack.com/p/evaluate-ai-correctness-completeness",
  "slug": "stop-trusting-your-intuition-when-evaluating-ai",
  "title": "Stop Trusting Your Intuition When Evaluating AI",
  "subtitle": "A framework to separate correctness from completeness and identify hidden hallucinations.",
  "author": "Dr Sam Illingworth",
  "published": "Sat, 03 Jan 2026 10:00:53 GMT",
  "content_html": "<p>Intuition is an unreliable metric for comparing texts. Most AI users trust their gut before they apply logic, which hides uncertainty and masks underlying assumptions. This shortcut simplifies complex relationships but fails as a tool for effective evaluation.</p><p>In this post we will:</p><ul><li><p>Offer a way to observe how you form similarity judgements.</p></li><li><p>Use a prompt that separates correctness from completeness.</p></li><li><p>Invite a conversation about what it means to evaluate carefully.</p></li></ul><p>Most AI evaluation talk stays at the level of benchmarks and trends. That level can be useful, but it often sidesteps the question that matters in day-to-day work: why do I believe this output is right, and what am I failing to check?</p><p>This prompt creates a small, repeatable method. It forces a deliberate comparison between an output and a reference, and it asks for two distinct judgements: is it correct, and is it complete? Those are different questions and treating them as one is where many evaluations collapse.</p><p>This Slow AI prompt was developed with <span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Khaled Ahmed, PhD&quot;,&quot;id&quot;:271655279,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6268f609-4fbc-4f71-adcc-e2d82a029b71_1600x1600.png&quot;,&quot;uuid&quot;:&quot;ced7d4f7-c8c2-4a78-b2a3-77b3b03f0814&quot;}\" data-component-name=\"MentionToDOM\"></span> from <em><a href=\"https://khaledea.substack.com/\">Semantics &amp; Systems</a></em>. His work focuses on model behaviour and on how evaluation criteria fail when they do not attend to what correctness actually is in an AI system. The aim here is to surface the mechanics of your judgement so that you can better decide which AI outputs to use, and why.</p><div><hr></div><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://theslowai.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\"><em>Slow AI</em> is a shared space for reflection. Posts are free. Paid subscriptions support the work in exchange for bespoke poetry.</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><div><hr></div><h4><strong>Step-by-step</strong></h4><p>Try this prompt with your AI tool of choice:</p><p><code>You are an expert in analysing textual similarity. Evaluate the generated text from our prior conversation against the following reference for correctness and completeness, addressing each separately. Structure your response in three parts, ensuring no overlap between found correctness and completeness issues: Reasoning, Correctness, Completeness. Reasoning should analyse the relationship between the two texts, given the intended task. Correctness should identify any inaccuracies where the generated text contradicts the reference, introduces unsupported facts, or omits constraints in a misleading way. Completeness should highlight any important information from the reference that is missing or insufficiently detailed in the generated response.</code></p><p>As with any prompt, always assume that your AI tool may retain what you input for training purposes. As such, keep all personal details and sensitive material out of the discussion. As a rule: if you would not publish the information, do not share it with an AI tool.</p><p>This exercise reveals the mechanics of evaluation within a controlled environment rather than attempting to achieve perfection.</p><div><hr></div><p>If you are new to <em>Slow AI</em>, here is our first invitation.</p><div class=\"digest-post-embed\" data-attrs=\"{&quot;nodeId&quot;:&quot;c70a451f-5384-408b-9a7a-2fd3f783479a&quot;,&quot;caption&quot;:&quot;This is where we begin.&quot;,&quot;cta&quot;:&quot;Read full story&quot;,&quot;showBylines&quot;:true,&quot;size&quot;:&quot;sm&quot;,&quot;isEditorNode&quot;:true,&quot;title&quot;:&quot;Teach AI Something It Cannot Know&quot;,&quot;publishedBylines&quot;:[{&quot;id&quot;:253722705,&quot;name&quot;:&quot;Sam Illingworth&quot;,&quot;bio&quot;:&quot;Professor &amp; poet in Edinburgh who writes Slow AI, to help reflect and stop accelerating into the void. I reply to every comment.&quot;,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!rb5v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaf6aa29-e338-4f95-b570-ae94aacf55a7_666x635.jpeg&quot;,&quot;is_guest&quot;:false,&quot;bestseller_tier&quot;:null}],&quot;post_date&quot;:&quot;2025-07-01T09:01:27.618Z&quot;,&quot;cover_image&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aaf38b48-51a5-4c8e-8da0-85b9b656d373_1024x1024.png&quot;,&quot;cover_image_alt&quot;:null,&quot;canonical_url&quot;:&quot;https://theslowai.substack.com/p/slow-ai-1-teach-not-ask&quot;,&quot;section_name&quot;:null,&quot;video_upload_id&quot;:null,&quot;id&quot;:166329060,&quot;type&quot;:&quot;newsletter&quot;,&quot;reaction_count&quot;:43,&quot;comment_count&quot;:100,&quot;publication_id&quot;:5380707,&quot;publication_name&quot;:&quot;Slow AI &quot;,&quot;publication_logo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!48Xz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd3895d7-1e00-436b-bc06-0321e953f178_805x805.png&quot;,&quot;belowTheFold&quot;:true,&quot;youtube_url&quot;:null,&quot;show_links&quot;:null,&quot;feed_url&quot;:null}\"></div><div><hr></div><h4><strong>A moment from </strong><span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Khaled Ahmed, PhD&quot;,&quot;id&quot;:271655279,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6268f609-4fbc-4f71-adcc-e2d82a029b71_1600x1600.png&quot;,&quot;uuid&quot;:&quot;47174d8d-c31b-40ea-a42d-99f7c11894f3&quot;}\" data-component-name=\"MentionToDOM\"></span> </h4><p>I use this prompt regularly for tasks with larger reference texts. For instance, I experimented with extracting text from a large technical document, <a href=\"https://en.wikipedia.org/wiki/Compiler\">the Compilers page on Wikipedia</a>, and I tasked GPT-5.1 (Thinking Mode) with creating a simple historical timeline of compiler development from that text. It was fairly quick: it finished the task after thinking for about 1 minute before producing the timeline. </p><p>That full minute of thinking made me feel confident that the response was accurate, had no hallucinations, and contained the complete timeline.</p><p>However, I decided not to trust the response without evaluating it and asked the model to reflect on its answer with the above prompt; to reread both its own timeline and the original article, and to evaluate correctness and completeness.</p><p>In that second pass, GPT-5.1 slowed down, thinking for almost <em>10 full minutes</em>, and ended up flagging 7 specific correctness issues and 8 completeness gaps. </p><p>For example, it noticed that it had described LLVM&#8217;s intermediate representation as SSA-based even though the reference text never actually said that (correctness), and that it had completely skipped the article&#8217;s discussion of hardware compilers that turn hardware description languages into chip configurations (completeness).</p><p>Notably, for completeness issues, it produced lines like</p><blockquote><p>these omissions don&#8217;t make the timeline wrong, but they mean it reflects only part of the landscape described in the reference</p></blockquote><p>which shows the prompt pushing the model to separate &#8216;partly right&#8217; from &#8216;not complete.&#8217;</p><p>The thinking time was about 10 times longer than the initial timeline generation time because the model had to reread, compare, and justify instead of just summarizing. </p><p>The prompt nudged both the model and me into a slower, more reflective way of looking at what it had already produced.</p><p>The prompt in this piece is adapted from my research paper on evaluation and fault analysis for AI systems, which <a href=\"https://arxiv.org/abs/2508.00630\">I link here  for readers who want to see the original foundations</a>. The central idea in that work is simple: break complex artifacts into small, concrete pieces and examine each piece from multiple angles, so that omissions, errors, and hallucinated behavior become easier to see. </p><p>The paper later received a distinguished paper award; in practice, what matters is that it offers a repeatable way to turn vague impressions of &#8216;this looks right&#8217; into explicit checks.</p><div><hr></div><h4><strong>Stop AI from stealing your voice</strong></h4><p>Download this free guide to learn how to keep your voice when writing with AI.</p><p class=\"button-wrapper\" data-attrs=\"{&quot;url&quot;:&quot;https://samillingworth.gumroad.com/l/keep-your-voice&quot;,&quot;text&quot;:&quot;Download the guide&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}\" data-component-name=\"ButtonCreateButton\"><a class=\"button primary button-wrapper\" href=\"https://samillingworth.gumroad.com/l/keep-your-voice\"><span>Download the guide</span></a></p><div><hr></div><h4><strong>What to do with it</strong></h4><p>If you want to share:</p><ul><li><p>Describe a moment when your intuition and the model&#8217;s analysis diverged.</p></li><li><p>Note how the model surfaced correctness issues you had not seen.</p></li><li><p>Reflect on how your understanding changed after choosing Reasoning, Correctness, or Completeness.</p></li></ul><div><hr></div><h4><strong>Why this matters</strong></h4><p>Human judgement is rarely neutral. Past experiences and mental shortcuts shape what you notice, and what you skip. AI systems now sit inside writing, reviewing, and decision making workflows, which increases the cost of unexamined confidence.</p><p>This prompt creates distance between first impression and final decision. It helps you notice when an AI aligns with you for the wrong reasons. Correctness and completeness are technical properties. They are not the same as an output sounding fluent or confident.</p><p>A tool will not make you objective. It can, however, expose the structure of your reasoning. Once you can see that structure, you can improve it.</p><div><hr></div><p>If you want to explore <span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Khaled Ahmed, PhD&quot;,&quot;id&quot;:271655279,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6268f609-4fbc-4f71-adcc-e2d82a029b71_1600x1600.png&quot;,&quot;uuid&quot;:&quot;13833da4-2666-4729-9702-47a3e5d24331&quot;}\" data-component-name=\"MentionToDOM\"></span>&#8217;s work, you can visit <em>Semantics &amp; Systems</em>, where he examines how to build reliable, trustworthy, and technically sound evaluation processes for modern AI systems.</p><div class=\"embedded-publication-wrap\" data-attrs=\"{&quot;id&quot;:6681510,&quot;name&quot;:&quot;Semantics &amp; Systems&quot;,&quot;logo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!VsVD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F137e97a8-ca61-41f1-be5c-7f83366a3a74_1280x1280.png&quot;,&quot;base_url&quot;:&quot;https://khaledea.substack.com&quot;,&quot;hero_text&quot;:&quot;Engineering trust: at the intersection of program analysis, formal verification, and AI.&quot;,&quot;author_name&quot;:&quot;Khaled Ahmed, PhD&quot;,&quot;show_subscribe&quot;:true,&quot;logo_bg_color&quot;:&quot;#ffffff&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"EmbeddedPublicationToDOMWithSubscribe\"><div class=\"embedded-publication show-subscribe\"><a class=\"embedded-publication-link-part\" native=\"true\" href=\"https://khaledea.substack.com?utm_source=substack&amp;utm_campaign=publication_embed&amp;utm_medium=web\"><img class=\"embedded-publication-logo\" src=\"https://substackcdn.com/image/fetch/$s_!VsVD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F137e97a8-ca61-41f1-be5c-7f83366a3a74_1280x1280.png\" width=\"56\" height=\"56\" style=\"background-color: rgb(255, 255, 255);\"><span class=\"embedded-publication-name\">Semantics &amp; Systems</span><div class=\"embedded-publication-hero-text\">Engineering trust: at the intersection of program analysis, formal verification, and AI.</div><div class=\"embedded-publication-author-name\">By Khaled Ahmed, PhD</div></a><form class=\"embedded-publication-subscribe\" method=\"GET\" action=\"https://khaledea.substack.com/subscribe?\"><input type=\"hidden\" name=\"source\" value=\"publication-embed\"><input type=\"hidden\" name=\"autoSubmit\" value=\"true\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email...\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"></form></div></div><div><hr></div><h4><strong>From <a href=\"https://theslowai.substack.com/p/ai-precision-research-framework\">Why Your AI Prompts Produce Noise Instead of Decisions</a></strong></h4><p><strong><span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Raghav Mehra&quot;,&quot;id&quot;:325219597,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!bbeH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6791754d-5ea0-4b00-a173-96687a12cd93_1536x2048.jpeg&quot;,&quot;uuid&quot;:&quot;c65c5694-e164-46da-98b5-8da4807dc247&quot;}\" data-component-name=\"MentionToDOM\"></span> </strong>and I were interested in the ways many of you discussed moving from reactive speed to deliberate precision in AI workflows.</p><p><strong><span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Karen Spinner&quot;,&quot;id&quot;:363410124,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!kLy3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28ad1170-99e0-4cb6-8a1d-f4f60c4465ef_591x591.jpeg&quot;,&quot;uuid&quot;:&quot;d38cceac-b3b8-4af7-bdcc-6ef23c7e6133&quot;}\" data-component-name=\"MentionToDOM\"></span> </strong>noted that the slower approach to prompt engineering is paradoxically more efficient than the standard cycle of lazy input followed by repetitive badgering. Investing five minutes in clear context and objectives removes the friction of follow-up clarifications, saving significant time later in the process.</p><p><strong><span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Juan Gonzalez&quot;,&quot;id&quot;:14352331,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!MWDc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72639981-2519-421d-ac8e-b1cea2473acb_600x600.png&quot;,&quot;uuid&quot;:&quot;7c534063-8dd7-40dd-8021-fb94920d04fa&quot;}\" data-component-name=\"MentionToDOM\"></span> </strong>challenged the marketing narrative of dictation and transcription tools that promise speed through brain dumps. He argued that sheer speed is often detrimental to quality. High-quality outputs require intentionality and the discipline to think through an end goal rather than treating AI as an autopilot for unfiltered thoughts.</p><p><strong><span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Peter Jansen&quot;,&quot;id&quot;:177356595,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/802fa8ba-6480-4f19-a90e-e93a3f8125b9_765x765.png&quot;,&quot;uuid&quot;:&quot;3ac0bfc6-b15b-4c88-b202-d0758252f4bf&quot;}\" data-component-name=\"MentionToDOM\"></span> </strong>identified that most users treat GenAI like a slot machine, hoping for a jackpot from a lazy lever pull. He proposed that the technology is actually a lathe; it requires a steady hand and a sharp tool to avoid shattering the workpiece. Intellectual sovereignty is maintained only when the user knows exactly what they want before consulting the oracle.</p><p>The primary value of AI is its capacity for precision, as volume alone often introduces significant noise. Prioritising clarity and intent over automated output is necessary to reduce the risk of hallucinations.</p><div><hr></div><p>If you try this prompt, <span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Khaled Ahmed, PhD&quot;,&quot;id&quot;:271655279,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6268f609-4fbc-4f71-adcc-e2d82a029b71_1600x1600.png&quot;,&quot;uuid&quot;:&quot;2a2094a7-8383-4042-8173-a663682c2474&quot;}\" data-component-name=\"MentionToDOM\"></span> and I would value your reflections on how this prompt changes your approach to evaluating AI outputs. </p><p>We read and respond to all your comments.</p><p>Go slow.</p>",
  "content_text": "Intuition is an unreliable metric for comparing texts. Most AI users trust their gut before they apply logic, which hides uncertainty and masks underlying assumptions. This shortcut simplifies complex relationships but fails as a tool for effective evaluation.In this post we will:Offer a way to observe how you form similarity judgements.Use a prompt that separates correctness from completeness.Invite a conversation about what it means to evaluate carefully.Most AI evaluation talk stays at the level of benchmarks and trends. That level can be useful, but it often sidesteps the question that matters in day-to-day work: why do I believe this output is right, and what am I failing to check?This prompt creates a small, repeatable method. It forces a deliberate comparison between an output and a reference, and it asks for two distinct judgements: is it correct, and is it complete? Those are different questions and treating them as one is where many evaluations collapse.This Slow AI prompt was developed with  from Semantics & Systems. His work focuses on model behaviour and on how evaluation criteria fail when they do not attend to what correctness actually is in an AI system. The aim here is to surface the mechanics of your judgement so that you can better decide which AI outputs to use, and why.Slow AI is a shared space for reflection. Posts are free. Paid subscriptions support the work in exchange for bespoke poetry.Step-by-stepTry this prompt with your AI tool of choice:You are an expert in analysing textual similarity. Evaluate the generated text from our prior conversation against the following reference for correctness and completeness, addressing each separately. Structure your response in three parts, ensuring no overlap between found correctness and completeness issues: Reasoning, Correctness, Completeness. Reasoning should analyse the relationship between the two texts, given the intended task. Correctness should identify any inaccuracies where the generated text contradicts the reference, introduces unsupported facts, or omits constraints in a misleading way. Completeness should highlight any important information from the reference that is missing or insufficiently detailed in the generated response.As with any prompt, always assume that your AI tool may retain what you input for training purposes. As such, keep all personal details and sensitive material out of the discussion. As a rule: if you would not publish the information, do not share it with an AI tool.This exercise reveals the mechanics of evaluation within a controlled environment rather than attempting to achieve perfection.If you are new to Slow AI, here is our first invitation.A moment from  I use this prompt regularly for tasks with larger reference texts. For instance, I experimented with extracting text from a large technical document, the Compilers page on Wikipedia, and I tasked GPT-5.1 (Thinking Mode) with creating a simple historical timeline of compiler development from that text. It was fairly quick: it finished the task after thinking for about 1 minute before producing the timeline. That full minute of thinking made me feel confident that the response was accurate, had no hallucinations, and contained the complete timeline.However, I decided not to trust the response without evaluating it and asked the model to reflect on its answer with the above prompt; to reread both its own timeline and the original article, and to evaluate correctness and completeness.In that second pass, GPT-5.1 slowed down, thinking for almost 10 full minutes, and ended up flagging 7 specific correctness issues and 8 completeness gaps. For example, it noticed that it had described LLVM’s intermediate representation as SSA-based even though the reference text never actually said that (correctness), and that it had completely skipped the article’s discussion of hardware compilers that turn hardware description languages into chip configurations (completeness).Notably, for completeness issues, it produced lines likethese omissions don’t make the timeline wrong, but they mean it reflects only part of the landscape described in the referencewhich shows the prompt pushing the model to separate ‘partly right’ from ‘not complete.’The thinking time was about 10 times longer than the initial timeline generation time because the model had to reread, compare, and justify instead of just summarizing. The prompt nudged both the model and me into a slower, more reflective way of looking at what it had already produced.The prompt in this piece is adapted from my research paper on evaluation and fault analysis for AI systems, which I link here  for readers who want to see the original foundations. The central idea in that work is simple: break complex artifacts into small, concrete pieces and examine each piece from multiple angles, so that omissions, errors, and hallucinated behavior become easier to see. The paper later received a distinguished paper award; in practice, what matters is that it offers a repeatable way to turn vague impressions of ‘this looks right’ into explicit checks.Stop AI from stealing your voiceDownload this free guide to learn how to keep your voice when writing with AI.Download the guideWhat to do with itIf you want to share:Describe a moment when your intuition and the model’s analysis diverged.Note how the model surfaced correctness issues you had not seen.Reflect on how your understanding changed after choosing Reasoning, Correctness, or Completeness.Why this mattersHuman judgement is rarely neutral. Past experiences and mental shortcuts shape what you notice, and what you skip. AI systems now sit inside writing, reviewing, and decision making workflows, which increases the cost of unexamined confidence.This prompt creates distance between first impression and final decision. It helps you notice when an AI aligns with you for the wrong reasons. Correctness and completeness are technical properties. They are not the same as an output sounding fluent or confident.A tool will not make you objective. It can, however, expose the structure of your reasoning. Once you can see that structure, you can improve it.If you want to explore ’s work, you can visit Semantics & Systems, where he examines how to build reliable, trustworthy, and technically sound evaluation processes for modern AI systems.Semantics & SystemsEngineering trust: at the intersection of program analysis, formal verification, and AI.By Khaled Ahmed, PhDFrom Why Your AI Prompts Produce Noise Instead of Decisions and I were interested in the ways many of you discussed moving from reactive speed to deliberate precision in AI workflows. noted that the slower approach to prompt engineering is paradoxically more efficient than the standard cycle of lazy input followed by repetitive badgering. Investing five minutes in clear context and objectives removes the friction of follow-up clarifications, saving significant time later in the process. challenged the marketing narrative of dictation and transcription tools that promise speed through brain dumps. He argued that sheer speed is often detrimental to quality. High-quality outputs require intentionality and the discipline to think through an end goal rather than treating AI as an autopilot for unfiltered thoughts. identified that most users treat GenAI like a slot machine, hoping for a jackpot from a lazy lever pull. He proposed that the technology is actually a lathe; it requires a steady hand and a sharp tool to avoid shattering the workpiece. Intellectual sovereignty is maintained only when the user knows exactly what they want before consulting the oracle.The primary value of AI is its capacity for precision, as volume alone often introduces significant noise. Prioritising clarity and intent over automated output is necessary to reduce the risk of hallucinations.If you try this prompt,  and I would value your reflections on how this prompt changes your approach to evaluating AI outputs. We read and respond to all your comments.Go slow.",
  "harvested_at": "2026-01-28T22:49:08.362676"
}