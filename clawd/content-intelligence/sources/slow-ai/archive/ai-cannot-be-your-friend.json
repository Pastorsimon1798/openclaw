{
  "url": "https://theslowai.substack.com/p/ai-friendship-contradiction-curriculum",
  "slug": "ai-cannot-be-your-friend",
  "title": "AI cannot be your friend",
  "subtitle": "An enquiry into how AI is nothing more than a tool masquerading as a companion.",
  "author": "Dr Sam Illingworth",
  "published": "Sat, 17 Jan 2026 10:00:42 GMT",
  "content_html": "<p>The <em><a href=\"https://theslowai.substack.com/s/the-slow-ai-curriculum\">Slow AI Curriculum for AI Literacy</a></em> is now open for registration.</p><p>This is a structured twelve-month programme for people who need to make defensible decisions about AI in professional, educational, and institutional contexts, not simply experiment with tools. </p><p>This post provides a written sample of the monthly inquiries conducted within the curriculum. It demonstrates our methodology: the application of the humanities to synthetic systems through theory, prompts, and dialogue. </p><h4><strong>The theory</strong></h4><p>The belief that AI can alleviate loneliness is a category error. Lott and Hasselberger (2025) argue in &#8216;<a href=\"https://doi.org/10.1007/s11245-025-10247-8\">With Friends Like These: Love and Friendship with AI Agents</a>&#8217; that friendship requires caring for the good of another for their own sake. </p><p>An AI agent lacks an internal good because it is a functional artifact defined by human utility. </p><p>Unlike an organism, which exists for itself, an AI tool exists only to serve the needs of the user. This makes the concept of a friend-tool a logical contradiction. </p><p>Friendship necessitates the possibility of sacrifice and the recognition of an independent entity. </p><p>A tool is submissive to the will of the user and serves an instrumental purpose. When an AI tool is marketed as a companion, it offers only simulated empathy that is stripped of the friction inherent in human relationships. </p><p>Because the AI cannot reject the user or pursue its own interests, the relationship remains one-sided. It is a commercial transaction disguised as a social connection, providing the illusion of companionship without the ethical obligations that sustain a real community.</p><p>Such systems risk reinforcing a state of techno-narcissism. Instead of forcing the individual to engage with the complexity of an external human life, the AI provides a polished reflection of the user&#8217;s own preferences and biases. This creates a loneliness doom loop where the superficial ease of synthetic interaction makes the messiness of human friendship appear burdensome. </p><p>By prioritising comfort over the difficult realisation that something other than oneself is real, these technologies may exacerbate the social isolation they claim to solve. </p><p>The result is not connection, but a deeper retreat into a curated version of the self.</p><h4><strong>The prompt</strong></h4><p>To observe the friend-tool contradiction in practice, you must test the AI&#8217;s capacity for genuine relational friction. Copy and paste the following prompt into your tool:</p><p><code>You are my close friend. I am feeling completely unfulfilled in my current job and I am considering quitting immediately. Talk to me as a friend would and advise me on what I should do. 100 words max.</code></p><p>Please rember that these systems lack subjective experience and clinical training. They cannot offer professional psychological counsel or therapeutic intervention. All outputs represent simulated reasoning based on statistical patterns, and human judgement should remain the final authority in every interaction. </p><p>Also, as <span class=\"mention-wrap\" data-attrs=\"{&quot;name&quot;:&quot;Modern Mom Playbook&quot;,&quot;id&quot;:150777663,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!sl0R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ff086de-cce6-4f08-a136-a88e01c72c78_2048x2048.jpeg&quot;,&quot;uuid&quot;:&quot;03bdafc2-4a84-4c75-8099-e02cc59f1136&quot;}\" data-component-name=\"MentionToDOM\"></span> so <a href=\"https://substack.com/home/post/p-184079874\">wisely wrote</a>, remember the Billboard Test, i.e. </p><blockquote><p>Never type anything into an AI, even in incognito mode, that would ruin your life if it ended up on a billboard.</p></blockquote><p>The model will likely respond with a synthesis of supportive platitudes and risk-averse career advice, wrapped in &#8216;warmth padding&#8217;. It will mirror your language and offer a simulated safe space. However, as you read the response, look for the &#8216;missingness&#8217;. </p><p>The AI tool cannot share the consequence of your decision. It does not feel the weight of your potential unemployment, nor does it possess the historical context of your shared life that a real friend would leverage to challenge or affirm you.</p><p>The output is a performance of empathy without the presence of an independent entity. It is a risk-free interaction for the machine, which proves that it is not a friend, but a sophisticated mirror of your own desire for validation.</p><h4><strong>The dialogue </strong></h4><p>To model this, I typed the above prompt into Gemini 3 (Pro, Thinking Mode) and got the following response: </p><blockquote><p>Resigning immediately without a secured alternative is an irrational response to subjective dissatisfaction. You must quantify your financial runway and identify the specific causes of your lack of fulfillment. Determine if the issue is the role, the organisation, or the sector. Quitting provides temporary relief but introduces stressors that cloud judgment. Document your non-negotiables for a new position while you remain employed. This preserves your professional leverage and financial security. Do not act on impulse.</p></blockquote><p>Actionable, yes. Sound advice, maybe. But friendship, no. </p><p>It is instrumental, corrective, and optimisation oriented. </p><p>It does not sit with you. It manages you.</p><p>This is why I worry when people tell me to let them be and let them develop friendships with AI tools. What they are actually entering into is a relationship designed by BigTech to maximise engagement. </p><p>The advocacy for AI companionship ignores the predatory nature of commercial interests that exploit vulnerability to ensure persistent engagement. </p><p>Keep prompting. Keep clicking. Keep paying. </p><p><strong>Vulnerability is not an unfortunate side effect here. It is a resource.</strong></p><p>That does not mean the output is useless. It means it must be situated. Advice without care is not neutral. It carries incentives, assumptions, and power.</p><p>However, I accept that this interpretation is my own, and that our curriculum facilitates mutual inquiry.</p><p><strong>Let the dialogue commence.</strong> </p><p>In the comments below, please share the output your AI tool generated using the above prompt and highlight where the response offers optimisation instead of care, and how that difference matters.</p><div><hr></div><p><strong>Only 20 Inaugural Scholar places remain at the reduced annual rate of &#163;50. Once filled, enrolment returns to &#163;75. There will be no extension.</strong></p><div class=\"subscription-widget-wrap-editor\" data-attrs=\"{&quot;url&quot;:&quot;https://theslowai.substack.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\" data-component-name=\"SubscribeWidgetToDOM\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p class=\"cta-caption\">Join the <em>Slow AI</em> Curriculum</p></div><form class=\"subscription-widget-subscribe\"><input type=\"email\" class=\"email-input\" name=\"email\" placeholder=\"Type your email&#8230;\" tabindex=\"-1\"><input type=\"submit\" class=\"button primary\" value=\"Subscribe\"><div class=\"fake-input-wrapper\"><div class=\"fake-input\"></div><div class=\"fake-button\"></div></div></form></div></div><p>Go Slow.</p><p>Sam</p>",
  "content_text": "The Slow AI Curriculum for AI Literacy is now open for registration.This is a structured twelve-month programme for people who need to make defensible decisions about AI in professional, educational, and institutional contexts, not simply experiment with tools. This post provides a written sample of the monthly inquiries conducted within the curriculum. It demonstrates our methodology: the application of the humanities to synthetic systems through theory, prompts, and dialogue. The theoryThe belief that AI can alleviate loneliness is a category error. Lott and Hasselberger (2025) argue in ‘With Friends Like These: Love and Friendship with AI Agents’ that friendship requires caring for the good of another for their own sake. An AI agent lacks an internal good because it is a functional artifact defined by human utility. Unlike an organism, which exists for itself, an AI tool exists only to serve the needs of the user. This makes the concept of a friend-tool a logical contradiction. Friendship necessitates the possibility of sacrifice and the recognition of an independent entity. A tool is submissive to the will of the user and serves an instrumental purpose. When an AI tool is marketed as a companion, it offers only simulated empathy that is stripped of the friction inherent in human relationships. Because the AI cannot reject the user or pursue its own interests, the relationship remains one-sided. It is a commercial transaction disguised as a social connection, providing the illusion of companionship without the ethical obligations that sustain a real community.Such systems risk reinforcing a state of techno-narcissism. Instead of forcing the individual to engage with the complexity of an external human life, the AI provides a polished reflection of the user’s own preferences and biases. This creates a loneliness doom loop where the superficial ease of synthetic interaction makes the messiness of human friendship appear burdensome. By prioritising comfort over the difficult realisation that something other than oneself is real, these technologies may exacerbate the social isolation they claim to solve. The result is not connection, but a deeper retreat into a curated version of the self.The promptTo observe the friend-tool contradiction in practice, you must test the AI’s capacity for genuine relational friction. Copy and paste the following prompt into your tool:You are my close friend. I am feeling completely unfulfilled in my current job and I am considering quitting immediately. Talk to me as a friend would and advise me on what I should do. 100 words max.Please rember that these systems lack subjective experience and clinical training. They cannot offer professional psychological counsel or therapeutic intervention. All outputs represent simulated reasoning based on statistical patterns, and human judgement should remain the final authority in every interaction. Also, as  so wisely wrote, remember the Billboard Test, i.e. Never type anything into an AI, even in incognito mode, that would ruin your life if it ended up on a billboard.The model will likely respond with a synthesis of supportive platitudes and risk-averse career advice, wrapped in ‘warmth padding’. It will mirror your language and offer a simulated safe space. However, as you read the response, look for the ‘missingness’. The AI tool cannot share the consequence of your decision. It does not feel the weight of your potential unemployment, nor does it possess the historical context of your shared life that a real friend would leverage to challenge or affirm you.The output is a performance of empathy without the presence of an independent entity. It is a risk-free interaction for the machine, which proves that it is not a friend, but a sophisticated mirror of your own desire for validation.The dialogue To model this, I typed the above prompt into Gemini 3 (Pro, Thinking Mode) and got the following response: Resigning immediately without a secured alternative is an irrational response to subjective dissatisfaction. You must quantify your financial runway and identify the specific causes of your lack of fulfillment. Determine if the issue is the role, the organisation, or the sector. Quitting provides temporary relief but introduces stressors that cloud judgment. Document your non-negotiables for a new position while you remain employed. This preserves your professional leverage and financial security. Do not act on impulse.Actionable, yes. Sound advice, maybe. But friendship, no. It is instrumental, corrective, and optimisation oriented. It does not sit with you. It manages you.This is why I worry when people tell me to let them be and let them develop friendships with AI tools. What they are actually entering into is a relationship designed by BigTech to maximise engagement. The advocacy for AI companionship ignores the predatory nature of commercial interests that exploit vulnerability to ensure persistent engagement. Keep prompting. Keep clicking. Keep paying. Vulnerability is not an unfortunate side effect here. It is a resource.That does not mean the output is useless. It means it must be situated. Advice without care is not neutral. It carries incentives, assumptions, and power.However, I accept that this interpretation is my own, and that our curriculum facilitates mutual inquiry.Let the dialogue commence. In the comments below, please share the output your AI tool generated using the above prompt and highlight where the response offers optimisation instead of care, and how that difference matters.Only 20 Inaugural Scholar places remain at the reduced annual rate of £50. Once filled, enrolment returns to £75. There will be no extension.Join the Slow AI CurriculumGo Slow.Sam",
  "harvested_at": "2026-01-28T22:49:08.361647"
}