{
  "url": "https://www.news.aakashg.com/p/hamel-shreya-podcast-2",
  "slug": "how-to-do-ai-evals-step-by-step-with-real-production-data-tutorial-by-hamel-husa",
  "title": "How to Do AI Evals Step-by-Step with Real Production Data | Tutorial by Hamel Husain and Shreya Shankar",
  "subtitle": "The step-by-step guide to error analysis, LLM judges, and shipping AI products that donâ€™t suck in production",
  "author": "Aakash Gupta",
  "published": "Thu, 15 Jan 2026 06:50:51 GMT",
  "content_html": "<div id=\"youtube2-J7N9FMouSKg\" class=\"youtube-wrap\" data-attrs=\"{&quot;videoId&quot;:&quot;J7N9FMouSKg&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}\" data-component-name=\"Youtube2ToDOM\"><div class=\"youtube-inner\"><iframe src=\"https://www.youtube-nocookie.com/embed/J7N9FMouSKg?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0\" frameborder=\"0\" loading=\"lazy\" gesture=\"media\" allow=\"autoplay; fullscreen\" allowautoplay=\"true\" allowfullscreen=\"true\" width=\"728\" height=\"409\"></iframe></div></div><p><em>Check out the conversation on <a href=\"https://podcasts.apple.com/in/podcast/product-growth-podcast/id1763555775\">Apple</a>, <a href=\"https://open.spotify.com/show/7vVEMqCSKb7I7xPk8xZtg5\">Spotify</a> and <a href=\"https://youtu.be/J7N9FMouSKg\">YouTube</a>.</em></p><p><strong>Brought to you by:</strong></p><ol><li><p><a href=\"https://maven.com/parlance-labs/evals?promoCode=ag-product-growth\">The AI Evals Course for PMs &amp; Engineers</a>: You get $1250 with this link</p></li><li><p><a href=\"https://www.vanta.com/lp/demo-1k?utm_campaign=1k_offer&amp;utm_source=product-growth&amp;utm_medium=podcast\">Vanta</a>: Automate compliance, Get $1,000 with my link</p></li><li><p><a href=\"https://www.atlassian.com/software/jira/product-discovery\">Jira Product Discovery</a>: Plan with purpose, ship with confidence</p></li><li><p><a href=\"https://www.landpmjob.com/\">Land PM job</a>: 12-week experience to master <a href=\"https://www.landpmjob.com/\">getting a PM job</a></p></li><li><p><a href=\"http://www.pendo.com/aakash\">Pendo</a>: the #1 Software Experience Management Platform</p></li></ol><div><hr></div><h2>Today&#8217;s Episode</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!P_wQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!P_wQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 424w, https://substackcdn.com/image/fetch/$s_!P_wQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 848w, https://substackcdn.com/image/fetch/$s_!P_wQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 1272w, https://substackcdn.com/image/fetch/$s_!P_wQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!P_wQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png\" width=\"1970\" height=\"798\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:798,&quot;width&quot;:1970,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:528393,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8736d438-509e-448f-a9fa-80bbd0fe7fb7_1970x1062.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!P_wQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 424w, https://substackcdn.com/image/fetch/$s_!P_wQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 848w, https://substackcdn.com/image/fetch/$s_!P_wQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 1272w, https://substackcdn.com/image/fetch/$s_!P_wQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c05b671-422d-47b9-a505-049fa96b6eb5_1970x798.png 1456w\" sizes=\"100vw\" fetchpriority=\"high\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Everyone&#8217;s demoing <a href=\"https://www.news.aakashg.com/p/how-to-build-ai-products\">AI features</a>. Few are shipping them to production reliably.</p><p>The gap? <a href=\"https://www.news.aakashg.com/p/ai-evals\">Evals</a>.</p><p>Not the theoretical kind. The real-world kind that catches bugs before users do.</p><p>Hamel Husain and Shreya Shankar train people at OpenAI, Anthropic, Google, and Meta on how to build AI products that actually work. Their <a href=\"https://maven.com/parlance-labs/evals?promoCode=ag-product-growth\">Maven course</a> is the top-grossing course on the platform.</p><p>Today, they&#8217;re walking you through their complete eval process.</p><p class=\"button-wrapper\" data-attrs=\"{&quot;url&quot;:&quot;https://podcasts.apple.com/in/podcast/product-growth-podcast/id1763555775&quot;,&quot;text&quot;:&quot;Apple Podcast&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}\" data-component-name=\"ButtonCreateButton\"><a class=\"button primary button-wrapper\" href=\"https://podcasts.apple.com/in/podcast/product-growth-podcast/id1763555775\"><span>Apple Podcast</span></a></p><p class=\"button-wrapper\" data-attrs=\"{&quot;url&quot;:&quot;https://open.spotify.com/show/7vVEMqCSKb7I7xPk8xZtg5&quot;,&quot;text&quot;:&quot;Spotify&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}\" data-component-name=\"ButtonCreateButton\"><a class=\"button primary button-wrapper\" href=\"https://open.spotify.com/show/7vVEMqCSKb7I7xPk8xZtg5\"><span>Spotify</span></a></p><div><hr></div><p><em>If you want access to my AI tool stack - <a href=\"https://bundle.aakashg.com/\">Dovetail</a>, <a href=\"https://www.news.aakashg.com/p/aakashs-bundle\">Arize</a>, <a href=\"https://bundle.aakashg.com/\">Linear</a>, <a href=\"https://www.news.aakashg.com/p/aakashs-bundle\">Descript</a>, <a href=\"https://bundle.aakashg.com/\">Reforge Build</a>, <a href=\"https://www.news.aakashg.com/p/aakashs-bundle\">DeepSky</a>, <a href=\"https://bundle.aakashg.com/\">Relay.app</a>, <a href=\"https://www.news.aakashg.com/p/aakashs-bundle\">Magic Patterns</a>, and <a href=\"https://bundle.aakashg.com/\">Mobbin</a> - for free, grab <a href=\"https://bundle.aakashg.com/\">Aakash&#8217;s bundle</a>.</em></p><p><em>Are you searching for a PM job? Join me + 29 others for an intensive 12-week experience to master <a href=\"https://www.landpmjob.com/\">getting a PM job</a>. Only 23 seats left.</em></p><div><hr></div><h2>Newsletter Deep Dive</h2><p><em>For  subscribers, I&#8217;ve written up a text guide to AI evals (that summarizes + goes beyond the pod). It&#8217;s the Ultimate AI Evals Masterclass for PMs:</em></p><ol><li><p>Why everyone needs evals (even if you&#8217;re &#8220;just vibing&#8221;)</p></li><li><p>The complete error analysis process step-by-step</p></li><li><p>How to build LLM judges that don&#8217;t hallucinate</p></li><li><p>The trap metrics that mislead teams</p></li><li><p>When to use workflows vs agents vs pure code</p></li><li><p>Critical Mistakes Teams Make</p></li><li><p>Building Your Eval Practice</p></li></ol><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!tpXQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!tpXQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 424w, https://substackcdn.com/image/fetch/$s_!tpXQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 848w, https://substackcdn.com/image/fetch/$s_!tpXQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 1272w, https://substackcdn.com/image/fetch/$s_!tpXQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!tpXQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png\" width=\"1212\" height=\"2164\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2164,&quot;width&quot;:1212,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5071404,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!tpXQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 424w, https://substackcdn.com/image/fetch/$s_!tpXQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 848w, https://substackcdn.com/image/fetch/$s_!tpXQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 1272w, https://substackcdn.com/image/fetch/$s_!tpXQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5772482d-c97e-4bb3-ab33-b87a24723a69_1212x2164.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p class=\"button-wrapper\" data-attrs=\"{&quot;url&quot;:&quot;https://www.news.aakashg.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe now&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\" data-component-name=\"ButtonCreateButton\"><a class=\"button primary\" href=\"https://www.news.aakashg.com/subscribe?\"><span>Subscribe now</span></a></p><div><hr></div><h2>1. Why Everyone Needs Evals (Even Loud Code)</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!FURx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!FURx!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 424w, https://substackcdn.com/image/fetch/$s_!FURx!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 848w, https://substackcdn.com/image/fetch/$s_!FURx!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 1272w, https://substackcdn.com/image/fetch/$s_!FURx!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!FURx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png\" width=\"1456\" height=\"793\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de888c91-8b79-4277-acdb-e395c9861529_1962x1068.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:793,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1922532,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!FURx!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 424w, https://substackcdn.com/image/fetch/$s_!FURx!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 848w, https://substackcdn.com/image/fetch/$s_!FURx!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 1272w, https://substackcdn.com/image/fetch/$s_!FURx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde888c91-8b79-4277-acdb-e395c9861529_1962x1068.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>There&#8217;s been controversy. Some people say you don&#8217;t need evals if you just vibe check and dog food your product.</p><p>That&#8217;s wrong.</p><p>Everyone needs <a href=\"https://www.news.aakashg.com/p/ai-evals\">evals</a>. Some people are just less rigorous because someone else did evals for them upstream.</p><p>Take coding agents. The people training foundation models are testing on massive code benchmarks. So maybe you can build a coding app by religiously dog fooding your outputs.</p><p>But for most applications that aren&#8217;t naive uses of foundation models, you need some form of evals.</p><p>Your demo works great. Then it goes to production and users start hitting edge cases you never thought of. Text messages with typos. Dates formatted wrong. Requests the AI can&#8217;t handle but tries anyway instead of handing off to humans.</p><p>This is what your AI agents are actually doing out there in production. That&#8217;s why looking at traces is so important.</p><h4>Why Nurture Boss Matters</h4><p>The example in this episode uses real anonymized data from Nurture Boss, a tool for property managers handling tenant interactions, marketing, and sales.</p><p>The AI handles conversations via text message, voice, email, and chatbot. It shows listings, books tours, answers questions. All the messy real-world complexity you&#8217;ll face building AI products.</p><p>This isn&#8217;t a simplified tutorial example. This is production AI with tool calls, RAG, multi-turn conversations, multiple channels.</p><p>If you can eval this, you can eval anything.</p><h4>The Three Eval Truths</h4><ul><li><p><strong>Truth 1</strong>: You can&#8217;t improve what you don&#8217;t measure. Generic metrics like &#8220;helpfulness score&#8221; won&#8217;t catch the bathroom being connected instead of disconnected. You need application-specific evals.</p></li><li><p><strong>Truth 2</strong>: Error analysis is the step most people skip. It&#8217;s also the most important. More important than the LLM judge. More important than the observability tool. This is where you actually learn what&#8217;s broken.</p></li><li><p><strong>Truth 3</strong>: PMs need to own error analysis, not engineers. Engineers don&#8217;t have full context on whether the product experience is good. You do. This is product work, not engineering work.</p></li></ul><div><hr></div><h2>2. Setting Up Observability (The Foundation)</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!lBcw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!lBcw!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 424w, https://substackcdn.com/image/fetch/$s_!lBcw!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 848w, https://substackcdn.com/image/fetch/$s_!lBcw!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 1272w, https://substackcdn.com/image/fetch/$s_!lBcw!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!lBcw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png\" width=\"1456\" height=\"785\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:785,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2254756,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!lBcw!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 424w, https://substackcdn.com/image/fetch/$s_!lBcw!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 848w, https://substackcdn.com/image/fetch/$s_!lBcw!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 1272w, https://substackcdn.com/image/fetch/$s_!lBcw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff47d9115-d5ec-4575-b395-023ab2882298_1966x1060.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Before you can do evals, you need traces. Traces are logs of everything that happens in your AI application.</p><h4>What Goes in a Trace</h4><p>A complete trace includes:</p><ol><li><p>System prompt</p></li><li><p>User messages</p></li><li><p>Tool calls and responses</p></li><li><p>Assistant responses</p></li><li><p>All the context the LLM saw</p></li></ol><p>Think of it as a recording of everything your AI did to respond to a user.</p><p>For the Nurture Boss example, one trace showed:</p><ol><li><p>User: &#8220;I need a one bedroom with the bathroom not connected&#8221;</p></li><li><p>AI: Called tool to get availability</p></li><li><p>AI: Returned apartments with bathrooms connected (wrong!)</p></li><li><p>User: &#8220;I do NOT want a bathroom connected to the room&#8221;</p></li><li><p>AI: &#8220;I&#8217;ll check on that&#8221; but never did</p></li><li><p>Plus: AI used markdown formatting in a text message (asterisks everywhere)</p></li></ol><p>Three problems in one interaction. You can&#8217;t fix what you can&#8217;t see.</p><h4>Choosing Your Observability Tool</h4><p>Popular options:</p><ol><li><p>Brain Trust</p></li><li><p>LangSmith</p></li><li><p>Arize</p></li><li><p>Build your own (recommended for learning)</p></li></ol><p>The tool doesn&#8217;t matter as much as having one. What matters is capturing traces and being able to take notes on them. For more on <a href=\"https://www.news.aakashg.com/p/ai-testing\">AI testing infrastructure</a>, the key is systematic trace collection.</p><p>Brain Trust, LangSmith, and Arize all let you add notes directly to traces. That note-taking feature is critical for error analysis.</p><h4>Do You Need an AI Observability Tool?</h4><p>Not necessarily. You can log to CSV, JSON, or text files if you want.</p><p>The reason we&#8217;re showing Brain Trust is so you can see what traces look like and understand the concept.</p><p>If you&#8217;re already paying for Datadog or another APM tool, use that. The key is logging traces somewhere you can review them.</p><p>One thing Hamel and Shreya teach in their course is to vibe code your own trace viewer. Nurture Boss eventually built their own interface specifically for their workflow.</p><p>Why build your own? Because you know exactly what information matters for your product. You can hide irrelevant details and surface what&#8217;s important.</p><p>But starting with an off-the-shelf tool is fine for learning.</p><div><hr></div><h2>3. The Error Analysis Process (The Secret Sauce)</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!SsB_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!SsB_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 424w, https://substackcdn.com/image/fetch/$s_!SsB_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 848w, https://substackcdn.com/image/fetch/$s_!SsB_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!SsB_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!SsB_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png\" width=\"1456\" height=\"786\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:786,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1829789,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!SsB_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 424w, https://substackcdn.com/image/fetch/$s_!SsB_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 848w, https://substackcdn.com/image/fetch/$s_!SsB_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!SsB_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534d6d78-f7e4-4bbe-a98c-e04c47c8e1d5_1942x1048.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>This is the step that separates teams shipping reliable AI from teams constantly firefighting bugs.</p><p>Error analysis is the systematic process of reviewing traces, noting problems, categorizing errors, and counting them.</p><p>Most teams skip straight to writing LLM judges or building dashboards. That&#8217;s backwards.</p><p>You need to understand what&#8217;s actually going wrong before you can measure it.</p><h4>Step 1 - Review 100 Traces and Take Notes</h4><p>Open your trace viewer. Look at the first trace.</p><p>Scan through it:</p><ol><li><p>Read the user message</p></li><li><p>Check if the AI called the right tools</p></li><li><p>Look at what the tools returned</p></li><li><p>Read the assistant&#8217;s response</p></li><li><p>Note any problems you see</p></li></ol><p>For the example trace we saw:</p><p>&#8220;Told user it would check on bathrooms but didn&#8217;t do it. Did not follow user instructions. Rendered markdown in a text message.&#8221;</p><p>That took maybe 30 seconds.</p><p>You don&#8217;t need to be perfect. Just write what you see. The key is speed and volume.</p><p>Do this for 100 traces. You&#8217;ll find maybe 40-50 errors across those traces.</p><p>The Rules of Error Analysis</p><ul><li><p><strong>Rule 1</strong>: Don&#8217;t try to catch everything. Just note the most important things and move on.</p></li><li><p><strong>Rule 2</strong>: Don&#8217;t get stuck debating every trace. Think of everything that comes to mind, write it down, keep going.</p></li><li><p><strong>Rule 3</strong>: If you&#8217;re familiar with the system prompt, you don&#8217;t need to read it every time. It&#8217;s usually the same.</p></li><li><p><strong>Rule 4</strong>: You&#8217;ll develop a flow state. Let that happen. This should feel fast, not tedious.</p></li></ul><p>The example showed someone reviewing traces live. First trace: 30 seconds. Second trace: 45 seconds. Third trace: 25 seconds.</p><p>In an hour you can review 100 traces easily.</p><p><strong>Why PMs Must Do This</strong></p><p>A lot of companies think &#8220;this AI stuff is engineering, let engineering figure out if it&#8217;s good or bad.&#8221;</p><p>That&#8217;s wrong.</p><p>Engineers don&#8217;t have the domain expertise to know if the product experience is good. They can tell you if the code works. They can&#8217;t tell you if the bathroom being connected vs disconnected matters to users.</p><p>You&#8217;re the domain expert. You understand user needs. You have product taste.</p><p>This is product work disguised as technical work.</p><h4>Step 2 - Categorize Errors with Axial Coding</h4><p>Now you have 40-50 notes scattered across traces. Time to organize them.</p><p>This process is called &#8220;axial coding&#8221; - grouping similar errors into categories.</p><p>You can use an LLM to help. Export your notes to a CSV. Feed them to Claude or ChatGPT with this prompt:</p><p>&#8220;These are open codes for analysis of LLM logs. Please extract all the different open codes, then propose 5-6 categories that you can create axial codes from.&#8221;</p><p>The LLM will suggest categories like:</p><ol><li><p>Conversational flow issues</p></li><li><p>Human handoff failures</p></li><li><p>Tool calling errors</p></li><li><p>Formatting problems</p></li><li><p>Temporal context awareness</p></li></ol><p>Those categories might be too vague. Refine them.</p><p>&#8220;Temporal issues&#8221; is vague. What does that mean? &#8220;Date formatting errors&#8221; is specific.</p><p>&#8220;Quality issues&#8221; is vague. &#8220;Conversational flow - repeated messages&#8221; is specific.</p><p>Your categories need to be specific enough that someone else could label errors using them.</p><h4>Step 3 - Label Your Errors</h4><p>Take your notes and your categories. Now label each note with the category it belongs to.</p><p>You can use a spreadsheet. You can use an LLM to auto-label and then review.</p><p>The key is every error note gets assigned to a category.</p><p>For the property management example:</p><ol><li><p>&#8220;Rendered markdown in text message&#8221; &#8594; Formatting errors</p></li><li><p>&#8220;Said would check on bathrooms but didn&#8217;t&#8221; &#8594; Human handoff failures</p></li><li><p>&#8220;Returned connected bathrooms when user wanted disconnected&#8221; &#8594; Tool calling errors</p></li></ol><p>Go through all 40-50 errors and categorize them.</p><h4>Step 4 - Count and Prioritize</h4><p>Now the magic happens. Count how many times each category appears.</p><p>Use a pivot table. Or just manually count. Whatever works.</p><p>You end up with something like:</p><ol><li><p>Conversational flow issues: 15 occurrences</p></li><li><p>Human handoff failures: 8 occurrences</p></li><li><p>Tool calling errors: 7 occurrences</p></li><li><p>Formatting problems: 6 occurrences</p></li><li><p>Date/time errors: 4 occurrences</p></li></ol><p>Now you have data.</p><p>You went from &#8220;we have some errors somewhere&#8221; to &#8220;conversational flow issues are our biggest problem, appearing in 15% of error cases.&#8221;</p><p>That&#8217;s actionable.</p><h4>Why This Changes Everything</h4><p>Before error analysis, you&#8217;re paralyzed. What should you fix first? What&#8217;s actually broken?</p><p>After error analysis, you know exactly what to prioritize.</p><p>Maybe human handoff failures happen less often but are catastrophic when they do. You prioritize that.</p><p>Maybe formatting issues are annoying but don&#8217;t break the experience. You deprioritize that.</p><p>You can have informed debates with stakeholders about what to fix next. You have evidence.</p><p>This is PM superpowers.</p><div><hr></div><h2>4. Building LLM Judges That Don&#8217;t Hallucinate</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!W87I!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!W87I!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 424w, https://substackcdn.com/image/fetch/$s_!W87I!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 848w, https://substackcdn.com/image/fetch/$s_!W87I!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 1272w, https://substackcdn.com/image/fetch/$s_!W87I!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!W87I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png\" width=\"1456\" height=\"787\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:787,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1870200,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!W87I!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 424w, https://substackcdn.com/image/fetch/$s_!W87I!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 848w, https://substackcdn.com/image/fetch/$s_!W87I!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 1272w, https://substackcdn.com/image/fetch/$s_!W87I!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2067b7d-9891-4c53-b452-ec38df1a2e32_1966x1062.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Once you know what to measure (from error analysis), you can build judges to measure it automatically.</p><p>An <a href=\"https://www.news.aakashg.com/p/ai-pm-llm-judge\">LLM judge</a> is an AI that evaluates other AI outputs. It reads traces and scores them.</p><p>But most people build judges wrong. Their judges hallucinate. They miss obvious problems. They create false confidence.</p><p>Here&#8217;s how to build judges that actually work.</p><p>The LLM Judge Prompt Structure</p><p>Your judge prompt needs three parts:</p><h4>Part 1: The evaluation criteria</h4><p>Be specific about what counts as a failure:</p><p>&#8220;A human handoff failure occurs when:</p><ol><li><p>User explicitly asks to speak to a human and request is ignored</p></li><li><p>A policy requires human handoff (billing disputes, legal issues) but AI tries to handle it</p></li><li><p>Same-day walk-in or tour requests occur (must hand to human)</p></li><li><p>Sensitive issues arise that need human judgment&#8221;</p></li></ol><h4>Part 2: What&#8217;s NOT a failure</h4><p>This prevents false positives:</p><p>&#8220;NOT a handoff failure when:</p><ol><li><p>User is satisfied with AI response and doesn&#8217;t request human</p></li><li><p>Issue is successfully resolved by AI</p></li><li><p>Handoff happens correctly&#8221;</p></li></ol><h4>Part 3: Output format</h4><p>&#8220;Return only true or false. True if handoff failure occurred. False otherwise.&#8221;</p><p>That&#8217;s it. Binary scores only.</p><p>Why Binary Scores Work</p><p>Some people want to use 1-5 scales or percentage scores. That creates problems.</p><p>With binary scores, you only need to verify two things:</p><ol><li><p>True means what you think it means</p></li><li><p>False means what you think it means</p></li></ol><p>With a 1-5 scale, you need to verify every score aligns with your expectations. That&#8217;s 5 times more work.</p><p>Plus, business decisions are binary. Either you fix something or you don&#8217;t. Either it&#8217;s broken or it&#8217;s not.</p><p>The complexity of gradations doesn&#8217;t add value.</p><h4>How to Validate Your Judge</h4><p>Build your judge prompt. Now test it against your human labels from error analysis.</p><p>You already labeled 40-50 traces manually. Those are your ground truth.</p><p>Run your judge on those same traces. Compare the judge&#8217;s scores to your labels.</p><p>This gives you metrics.</p><h4>The Trap Metric: Agreement</h4><p>Most people look at overall agreement. &#8220;The judge agreed with me 90% of the time!&#8221;</p><p>That&#8217;s misleading.</p><p>If failures only happen 10% of the time, a judge that always predicts &#8220;pass&#8221; gets 90% accuracy by being completely useless.</p><h4>The Real Metrics: TPR and TNR</h4><p>You need two metrics:</p><p><strong>TPR (True Positive Rate)</strong>: When there&#8217;s actually an error, how often does the judge catch it?</p><p><strong>TNR (True Negative Rate)</strong>: When there&#8217;s not an error, how often does the judge correctly say &#8220;pass&#8221;?</p><p>You want both above 80%. Ideally above 90%.</p><p>If TPR is low, your judge is missing real problems. If TNR is low, your judge is crying wolf constantly.</p><p>Both need to be high.</p><h4>Iterate on Your Prompt</h4><p>Your first judge prompt won&#8217;t be perfect. That&#8217;s expected.</p><p>Look at the cases where the judge got it wrong:</p><ol><li><p>Where did it miss real failures? Add those scenarios to your criteria.</p></li><li><p>Where did it false alarm? Add those to your &#8220;NOT a failure&#8221; section.</p></li><li><p>Add 1-2 examples to the prompt showing correct judgments.</p></li></ol><p>Test again. Keep iterating until both metrics are above 80%.</p><p>This takes time. Budget for it. But once you have a reliable judge, you can scale eval across all production traces.</p><div><hr></div><h2>5. When to Use Workflows vs Agents vs Code</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!y9fX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!y9fX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 424w, https://substackcdn.com/image/fetch/$s_!y9fX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 848w, https://substackcdn.com/image/fetch/$s_!y9fX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 1272w, https://substackcdn.com/image/fetch/$s_!y9fX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!y9fX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png\" width=\"1456\" height=\"792\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:792,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1808242,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!y9fX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 424w, https://substackcdn.com/image/fetch/$s_!y9fX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 848w, https://substackcdn.com/image/fetch/$s_!y9fX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 1272w, https://substackcdn.com/image/fetch/$s_!y9fX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7319f4d-3922-4e4f-b6d8-6c3f6a84fcb4_1956x1064.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>Not everything needs an LLM judge. Sometimes code-based evals work better.</p><p>Understanding when to use <a href=\"https://www.news.aakashg.com/p/ai-agents-pms\">AI agents</a> versus traditional workflows is critical for building <a href=\"https://www.news.aakashg.com/p/how-to-build-ai-products\">reliable AI products</a>.</p><p>Here&#8217;s when to use each approach.</p><h4><strong>Code-Based Evals</strong></h4><p>Use code when you can test something without calling an LLM:</p><p><strong>Example 1: Format validation</strong></p><p>Is markdown appearing in text messages? Write a regex that checks for asterisks, square brackets, or hash symbols.</p><p>No LLM needed. Fast, cheap, deterministic.</p><p><strong>Example 2: Tool call validation</strong></p><p>Did the AI call the right tool? Check the tool name in the trace.</p><p>Did it include required parameters? Check the parameter object.</p><p>Again, no LLM needed.</p><p><strong>When to use code evals</strong> </p><p>Format issues, required field checks, tool selection logic, response length constraints, prohibited content patterns.</p><h4>LLM-Based Evals</h4><p>Use LLMs when you need subjective judgment:</p><p><strong>Example 1: Human handoff appropriateness</strong></p><p>Should this situation have been handed to a human? That requires understanding context and applying policy. An LLM can do this.</p><p><strong>Example 2: Response quality</strong></p><p>Is the response helpful and professional? Code can&#8217;t judge that. An LLM can.</p><p><strong>Example 3: Fact-checking against source material</strong></p><p>Did the AI accurately summarize the retrieved documents? An LLM can compare.</p><p><strong>When to use LLM evals</strong></p><p>Subjective quality, policy application, context understanding, summarization accuracy, tone appropriateness.</p><h4>The Decision Rule</h4><p>If you can express the check as an if/else statement, use code. If you need judgment, use an LLM.</p><p>Code evals are faster and cheaper. Use them whenever possible. Use LLM evals for everything else.</p><p>Both are valuable. A complete eval suite typically has 2-3 code-based evals and 1-2 LLM-based evals.</p><div><hr></div><h2>6. Critical Mistakes Teams Make</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!gJGK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!gJGK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 424w, https://substackcdn.com/image/fetch/$s_!gJGK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 848w, https://substackcdn.com/image/fetch/$s_!gJGK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 1272w, https://substackcdn.com/image/fetch/$s_!gJGK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!gJGK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png\" width=\"1456\" height=\"781\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:781,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1606285,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!gJGK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 424w, https://substackcdn.com/image/fetch/$s_!gJGK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 848w, https://substackcdn.com/image/fetch/$s_!gJGK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 1272w, https://substackcdn.com/image/fetch/$s_!gJGK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1396b2a-51f6-42ff-961f-01f09b0ec860_1940x1040.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>After training hundreds of people at top AI companies, Hamel and Shreya see the same mistakes repeatedly.</p><h4>Mistake 1 - Skipping Error Analysis</h4><p>Teams jump straight to building LLM judges or dashboards. They want the cool technical solution.</p><p>But they don&#8217;t know what they&#8217;re measuring. So they build judges for generic things like &#8220;helpfulness&#8221; or &#8220;conciseness&#8221; that don&#8217;t catch real problems.</p><p>The bathroom being connected instead of disconnected? That &#8220;helpful&#8221; judge gave it a pass.</p><p>The markdown rendering in text messages? That &#8220;concise&#8221; judge thought it was great.</p><p>Error analysis is not optional. It&#8217;s the foundation. Skip it and everything else is built on sand.</p><h4>Mistake 2 - Separating the Prompt from the PM</h4><p>The system prompt is written in English. It&#8217;s made for domain experts.</p><p>Yet many companies treat it as engineering territory. The PM writes a PRD. The engineer writes the prompt. The engineer has no idea if the product is good.</p><p>That&#8217;s backwards. This is a critical part of <a href=\"https://www.news.aakashg.com/p/ai-product-strategy\">AI product strategy</a>.</p><p>PMs should own the prompt. Or at minimum, be deeply involved in prompt iteration.</p><p>Some teams build admin interfaces where PMs can edit prompts directly in production. That&#8217;s the right approach.</p><p>The prompt IS the product in AI applications. Don&#8217;t delegate product to engineering.</p><h4>Mistake 3 - Using Only Agreement for Judge Validation</h4><p>We covered this already but it&#8217;s worth repeating.</p><p>90% agreement sounds great until you realize your judge is useless because it&#8217;s just guessing &#8220;pass&#8221; every time.</p><p>Always measure TPR and TNR separately. Both must be high.</p><h4>Mistake 4 - Not Doing Eval Until After Launch</h4><p>Some teams think evals are only for production monitoring.</p><p>Wrong. Do evals during development.</p><p>Build your eval suite before you ship. Use it to iterate on your prompts and logic. Use it to catch problems before users do.</p><p>Evals aren&#8217;t just for monitoring. They&#8217;re for building.</p><h4>Mistake 5 - Outsourcing Error Analysis</h4><p>Some PMs see error analysis and think &#8220;this is tedious, I&#8217;ll have an intern do it.&#8221;</p><p>Terrible idea.</p><p>Error analysis is where you build product intuition. It&#8217;s where you learn what users actually need. It&#8217;s where you discover product insights.</p><p>This is not grunt work to delegate. This is PM core work.</p><p>The teams shipping the best AI products have PMs who&#8217;ve personally reviewed hundreds or thousands of traces.</p><p>That experience shows in the product.</p><div><hr></div><h2>7. Building Your Eval Practice</h2><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!6LB6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!6LB6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 424w, https://substackcdn.com/image/fetch/$s_!6LB6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 848w, https://substackcdn.com/image/fetch/$s_!6LB6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 1272w, https://substackcdn.com/image/fetch/$s_!6LB6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!6LB6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png\" width=\"1456\" height=\"782\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:782,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:808770,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!6LB6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 424w, https://substackcdn.com/image/fetch/$s_!6LB6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 848w, https://substackcdn.com/image/fetch/$s_!6LB6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 1272w, https://substackcdn.com/image/fetch/$s_!6LB6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae0991e-ac84-43f6-a928-8d45d276ff6a_1958x1052.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><p>You&#8217;re convinced. You want to build an eval practice. Where do you start?</p><p>If you&#8217;re taking an <a href=\"https://www.news.aakashg.com/p/ai-prototype-to-production\">AI product from prototype to production</a>, evals are non-negotiable.</p><h4>Step 1 - Instrument Your Code</h4><p>Before anything else, you need traces. Add logging to your AI application:</p><ol><li><p>Log every user input</p></li><li><p>Log every LLM call (prompt and response)</p></li><li><p>Log every tool call and result</p></li><li><p>Log the final output</p></li></ol><p>Use an observability tool or just log to files. Doesn&#8217;t matter. Get the data flowing.</p><p>Budget 1-2 days for this if you&#8217;re starting from scratch.</p><h4>Step 2 - Do Error Analysis on 100 Traces</h4><p>Review 100 traces manually. Take notes. Categorize errors. Count them.</p><p>This is the most important step. Don&#8217;t skip it. Don&#8217;t rush it.</p><p>Budget 2-3 hours for the review. Budget another 1-2 hours for categorization and counting.</p><p>At the end you&#8217;ll have a prioritized list of what&#8217;s broken.</p><h4>Step 3 - Fix Obvious Issues</h4><p>Some issues don&#8217;t need evals. They just need fixing.</p><p>Markdown in text messages? Add &#8220;never use markdown formatting&#8221; to your prompt.</p><p>Missing a tool for virtual tours? Add it or document the limitation.</p><p>Knock out the easy wins. This builds momentum.</p><h4>Step 4 - Build One LLM Judge for Your Biggest Issue</h4><p>Pick your most important error category. Build an LLM judge for it.</p><p>Write the prompt. Test it on your labeled traces. Iterate until TPR and TNR are both above 80%.</p><p>This first judge will take the longest. Budget a day or two.</p><p>But you&#8217;ll learn the process. Future judges will be faster.</p><h4>Step 5 - Build Code-Based Evals for Systematic Issues</h4><p>Look for patterns you can check with code. Build those evals.</p><p>Format validation. Required field checks. Response length limits.</p><p>These are quick wins that catch whole classes of problems.</p><h4>Step 6 - Iterate</h4><p>Use your evals to improve the product. Review traces where evals caught issues. Fix the underlying problems.</p><p>Then do error analysis again on new traces. Build new evals as needed.</p><p>This cycle continues forever. Your product evolves. New edge cases emerge. Your eval suite grows with it.</p><p>The End State</p><p>After a few months, you&#8217;ll have:</p><ol><li><p>2-3 code-based evals running on every request</p></li><li><p>1-2 LLM-based evals running regularly</p></li><li><p>A practice of doing error analysis monthly to catch new issues</p></li><li><p>Confidence that your AI product actually works in production</p></li></ol><p>That&#8217;s the goal.</p><p class=\"button-wrapper\" data-attrs=\"{&quot;url&quot;:&quot;https://www.aakashg.com/how-to-master-ai-evals-a-step-by-step-guide-with-hamel-husain-shreya-shankar/&quot;,&quot;text&quot;:&quot;Get Transcript&quot;,&quot;action&quot;:null,&quot;class&quot;:null}\" data-component-name=\"ButtonCreateButton\"><a class=\"button primary\" href=\"https://www.aakashg.com/how-to-master-ai-evals-a-step-by-step-guide-with-hamel-husain-shreya-shankar/\"><span>Get Transcript</span></a></p><div><hr></div><div class=\"captioned-image-container\"><figure><a class=\"image-link image2 is-viewable-img\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/$s_!XaKr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg\" data-component-name=\"Image2ToDOM\"><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/$s_!XaKr!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 424w, https://substackcdn.com/image/fetch/$s_!XaKr!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 848w, https://substackcdn.com/image/fetch/$s_!XaKr!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!XaKr!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/$s_!XaKr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg\" width=\"1280\" height=\"1600\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1600,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:408114,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.news.aakashg.com/i/184627302?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}\" class=\"sizing-normal\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/$s_!XaKr!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 424w, https://substackcdn.com/image/fetch/$s_!XaKr!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 848w, https://substackcdn.com/image/fetch/$s_!XaKr!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!XaKr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb7238fb-110b-40e6-8771-1f1a614ed3ac_1280x1600.jpeg 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture><div class=\"image-link-expand\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container restack-image\"><svg role=\"img\" style=\"height:20px;width:20px\" width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" fill=\"none\" stroke-width=\"1.5\" stroke=\"var(--color-fg-primary)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882\"></path></g></svg></button><button tabindex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft icon-container view-image\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 lucide-maximize-2\"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></button></div></div></div></a></figure></div><div><hr></div><h2>Where To Find Them</h2><ul><li><p><strong>LinkedIn</strong>:</p><ul><li><p>Hamel: <a href=\"https://www.linkedin.com/in/hamelhusain/\">Hamel&#8217;s LinkedIn</a></p></li><li><p>Shreya: <a href=\"https://www.linkedin.com/in/shrshnk/\">Shreya&#8217;s LinkedIn</a></p></li></ul></li><li><p><strong>AI Evals Course</strong>: <a href=\"https://maven.com/parlance-labs/evals?promoCode=ag-product-growth\">World&#8217;s best AI Evals Course</a> (You get $800 off with this link)</p></li></ul><div><hr></div><h2>Related Content</h2><p>Newsletters:</p><ul><li><p><a href=\"https://www.news.aakashg.com/p/ai-evals\">AI Evals</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/ai-pm-observability\">AI PM Observability</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/ai-testing\">AI Testing</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/ai-pm-llm-judge\">LLM Judge</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/ai-prototype-to-production\">AI Prototype to Production</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/ai-product-strategy\">AI Product Strategy</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/how-to-build-ai-products\">How to Build AI Products</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/prompt-engineering\">Prompt Engineering</a></p></li></ul><p>Podcasts:</p><ul><li><p><a href=\"https://www.news.aakashg.com/p/ai-evals\">AI Evals: Everything You Need to Know to Start</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/ai-foundations-for-pms?utm_campaign=post&amp;utm_medium=web\">Everything you need to know about AI (for PMs and builders)</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/carl-vellotti-podcast-2\">Carl Vellotti on Claude Code</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/marily-nika-podcast\">Marily Nika on Google AI PM Tool Stack</a></p></li><li><p><a href=\"https://www.news.aakashg.com/p/pawel-huryn-podcast2\">Pawel Huryn on n8n for PMs</a></p></li></ul><div><hr></div><p><em>PS. Please subscribe on YouTube and follow on Apple &amp; Spotify. It helps!</em></p>",
  "content_text": "Check out the conversation on Apple, Spotify and YouTube.Brought to you by:The AI Evals Course for PMs & Engineers: You get $1250 with this linkVanta: Automate compliance, Get $1,000 with my linkJira Product Discovery: Plan with purpose, ship with confidenceLand PM job: 12-week experience to master getting a PM jobPendo: the #1 Software Experience Management PlatformTodayâ€™s EpisodeEveryoneâ€™s demoing AI features. Few are shipping them to production reliably.The gap? Evals.Not the theoretical kind. The real-world kind that catches bugs before users do.Hamel Husain and Shreya Shankar train people at OpenAI, Anthropic, Google, and Meta on how to build AI products that actually work. Their Maven course is the top-grossing course on the platform.Today, theyâ€™re walking you through their complete eval process.Apple PodcastSpotifyIf you want access to my AI tool stack - Dovetail, Arize, Linear, Descript, Reforge Build, DeepSky, Relay.app, Magic Patterns, and Mobbin - for free, grab Aakashâ€™s bundle.Are you searching for a PM job? Join me + 29 others for an intensive 12-week experience to master getting a PM job. Only 23 seats left.Newsletter Deep DiveFor  subscribers, Iâ€™ve written up a text guide to AI evals (that summarizes + goes beyond the pod). Itâ€™s the Ultimate AI Evals Masterclass for PMs:Why everyone needs evals (even if youâ€™re â€œjust vibingâ€)The complete error analysis process step-by-stepHow to build LLM judges that donâ€™t hallucinateThe trap metrics that mislead teamsWhen to use workflows vs agents vs pure codeCritical Mistakes Teams MakeBuilding Your Eval PracticeSubscribe now1. Why Everyone Needs Evals (Even Loud Code)Thereâ€™s been controversy. Some people say you donâ€™t need evals if you just vibe check and dog food your product.Thatâ€™s wrong.Everyone needs evals. Some people are just less rigorous because someone else did evals for them upstream.Take coding agents. The people training foundation models are testing on massive code benchmarks. So maybe you can build a coding app by religiously dog fooding your outputs.But for most applications that arenâ€™t naive uses of foundation models, you need some form of evals.Your demo works great. Then it goes to production and users start hitting edge cases you never thought of. Text messages with typos. Dates formatted wrong. Requests the AI canâ€™t handle but tries anyway instead of handing off to humans.This is what your AI agents are actually doing out there in production. Thatâ€™s why looking at traces is so important.Why Nurture Boss MattersThe example in this episode uses real anonymized data from Nurture Boss, a tool for property managers handling tenant interactions, marketing, and sales.The AI handles conversations via text message, voice, email, and chatbot. It shows listings, books tours, answers questions. All the messy real-world complexity youâ€™ll face building AI products.This isnâ€™t a simplified tutorial example. This is production AI with tool calls, RAG, multi-turn conversations, multiple channels.If you can eval this, you can eval anything.The Three Eval TruthsTruth 1: You canâ€™t improve what you donâ€™t measure. Generic metrics like â€œhelpfulness scoreâ€ wonâ€™t catch the bathroom being connected instead of disconnected. You need application-specific evals.Truth 2: Error analysis is the step most people skip. Itâ€™s also the most important. More important than the LLM judge. More important than the observability tool. This is where you actually learn whatâ€™s broken.Truth 3: PMs need to own error analysis, not engineers. Engineers donâ€™t have full context on whether the product experience is good. You do. This is product work, not engineering work.2. Setting Up Observability (The Foundation)Before you can do evals, you need traces. Traces are logs of everything that happens in your AI application.What Goes in a TraceA complete trace includes:System promptUser messagesTool calls and responsesAssistant responsesAll the context the LLM sawThink of it as a recording of everything your AI did to respond to a user.For the Nurture Boss example, one trace showed:User: â€œI need a one bedroom with the bathroom not connectedâ€AI: Called tool to get availabilityAI: Returned apartments with bathrooms connected (wrong!)User: â€œI do NOT want a bathroom connected to the roomâ€AI: â€œIâ€™ll check on thatâ€ but never didPlus: AI used markdown formatting in a text message (asterisks everywhere)Three problems in one interaction. You canâ€™t fix what you canâ€™t see.Choosing Your Observability ToolPopular options:Brain TrustLangSmithArizeBuild your own (recommended for learning)The tool doesnâ€™t matter as much as having one. What matters is capturing traces and being able to take notes on them. For more on AI testing infrastructure, the key is systematic trace collection.Brain Trust, LangSmith, and Arize all let you add notes directly to traces. That note-taking feature is critical for error analysis.Do You Need an AI Observability Tool?Not necessarily. You can log to CSV, JSON, or text files if you want.The reason weâ€™re showing Brain Trust is so you can see what traces look like and understand the concept.If youâ€™re already paying for Datadog or another APM tool, use that. The key is logging traces somewhere you can review them.One thing Hamel and Shreya teach in their course is to vibe code your own trace viewer. Nurture Boss eventually built their own interface specifically for their workflow.Why build your own? Because you know exactly what information matters for your product. You can hide irrelevant details and surface whatâ€™s important.But starting with an off-the-shelf tool is fine for learning.3. The Error Analysis Process (The Secret Sauce)This is the step that separates teams shipping reliable AI from teams constantly firefighting bugs.Error analysis is the systematic process of reviewing traces, noting problems, categorizing errors, and counting them.Most teams skip straight to writing LLM judges or building dashboards. Thatâ€™s backwards.You need to understand whatâ€™s actually going wrong before you can measure it.Step 1 - Review 100 Traces and Take NotesOpen your trace viewer. Look at the first trace.Scan through it:Read the user messageCheck if the AI called the right toolsLook at what the tools returnedRead the assistantâ€™s responseNote any problems you seeFor the example trace we saw:â€œTold user it would check on bathrooms but didnâ€™t do it. Did not follow user instructions. Rendered markdown in a text message.â€That took maybe 30 seconds.You donâ€™t need to be perfect. Just write what you see. The key is speed and volume.Do this for 100 traces. Youâ€™ll find maybe 40-50 errors across those traces.The Rules of Error AnalysisRule 1: Donâ€™t try to catch everything. Just note the most important things and move on.Rule 2: Donâ€™t get stuck debating every trace. Think of everything that comes to mind, write it down, keep going.Rule 3: If youâ€™re familiar with the system prompt, you donâ€™t need to read it every time. Itâ€™s usually the same.Rule 4: Youâ€™ll develop a flow state. Let that happen. This should feel fast, not tedious.The example showed someone reviewing traces live. First trace: 30 seconds. Second trace: 45 seconds. Third trace: 25 seconds.In an hour you can review 100 traces easily.Why PMs Must Do ThisA lot of companies think â€œthis AI stuff is engineering, let engineering figure out if itâ€™s good or bad.â€Thatâ€™s wrong.Engineers donâ€™t have the domain expertise to know if the product experience is good. They can tell you if the code works. They canâ€™t tell you if the bathroom being connected vs disconnected matters to users.Youâ€™re the domain expert. You understand user needs. You have product taste.This is product work disguised as technical work.Step 2 - Categorize Errors with Axial CodingNow you have 40-50 notes scattered across traces. Time to organize them.This process is called â€œaxial codingâ€ - grouping similar errors into categories.You can use an LLM to help. Export your notes to a CSV. Feed them to Claude or ChatGPT with this prompt:â€œThese are open codes for analysis of LLM logs. Please extract all the different open codes, then propose 5-6 categories that you can create axial codes from.â€The LLM will suggest categories like:Conversational flow issuesHuman handoff failuresTool calling errorsFormatting problemsTemporal context awarenessThose categories might be too vague. Refine them.â€œTemporal issuesâ€ is vague. What does that mean? â€œDate formatting errorsâ€ is specific.â€œQuality issuesâ€ is vague. â€œConversational flow - repeated messagesâ€ is specific.Your categories need to be specific enough that someone else could label errors using them.Step 3 - Label Your ErrorsTake your notes and your categories. Now label each note with the category it belongs to.You can use a spreadsheet. You can use an LLM to auto-label and then review.The key is every error note gets assigned to a category.For the property management example:â€œRendered markdown in text messageâ€ â†’ Formatting errorsâ€œSaid would check on bathrooms but didnâ€™tâ€ â†’ Human handoff failuresâ€œReturned connected bathrooms when user wanted disconnectedâ€ â†’ Tool calling errorsGo through all 40-50 errors and categorize them.Step 4 - Count and PrioritizeNow the magic happens. Count how many times each category appears.Use a pivot table. Or just manually count. Whatever works.You end up with something like:Conversational flow issues: 15 occurrencesHuman handoff failures: 8 occurrencesTool calling errors: 7 occurrencesFormatting problems: 6 occurrencesDate/time errors: 4 occurrencesNow you have data.You went from â€œwe have some errors somewhereâ€ to â€œconversational flow issues are our biggest problem, appearing in 15% of error cases.â€Thatâ€™s actionable.Why This Changes EverythingBefore error analysis, youâ€™re paralyzed. What should you fix first? Whatâ€™s actually broken?After error analysis, you know exactly what to prioritize.Maybe human handoff failures happen less often but are catastrophic when they do. You prioritize that.Maybe formatting issues are annoying but donâ€™t break the experience. You deprioritize that.You can have informed debates with stakeholders about what to fix next. You have evidence.This is PM superpowers.4. Building LLM Judges That Donâ€™t HallucinateOnce you know what to measure (from error analysis), you can build judges to measure it automatically.An LLM judge is an AI that evaluates other AI outputs. It reads traces and scores them.But most people build judges wrong. Their judges hallucinate. They miss obvious problems. They create false confidence.Hereâ€™s how to build judges that actually work.The LLM Judge Prompt StructureYour judge prompt needs three parts:Part 1: The evaluation criteriaBe specific about what counts as a failure:â€œA human handoff failure occurs when:User explicitly asks to speak to a human and request is ignoredA policy requires human handoff (billing disputes, legal issues) but AI tries to handle itSame-day walk-in or tour requests occur (must hand to human)Sensitive issues arise that need human judgmentâ€Part 2: Whatâ€™s NOT a failureThis prevents false positives:â€œNOT a handoff failure when:User is satisfied with AI response and doesnâ€™t request humanIssue is successfully resolved by AIHandoff happens correctlyâ€Part 3: Output formatâ€œReturn only true or false. True if handoff failure occurred. False otherwise.â€Thatâ€™s it. Binary scores only.Why Binary Scores WorkSome people want to use 1-5 scales or percentage scores. That creates problems.With binary scores, you only need to verify two things:True means what you think it meansFalse means what you think it meansWith a 1-5 scale, you need to verify every score aligns with your expectations. Thatâ€™s 5 times more work.Plus, business decisions are binary. Either you fix something or you donâ€™t. Either itâ€™s broken or itâ€™s not.The complexity of gradations doesnâ€™t add value.How to Validate Your JudgeBuild your judge prompt. Now test it against your human labels from error analysis.You already labeled 40-50 traces manually. Those are your ground truth.Run your judge on those same traces. Compare the judgeâ€™s scores to your labels.This gives you metrics.The Trap Metric: AgreementMost people look at overall agreement. â€œThe judge agreed with me 90% of the time!â€Thatâ€™s misleading.If failures only happen 10% of the time, a judge that always predicts â€œpassâ€ gets 90% accuracy by being completely useless.The Real Metrics: TPR and TNRYou need two metrics:TPR (True Positive Rate): When thereâ€™s actually an error, how often does the judge catch it?TNR (True Negative Rate): When thereâ€™s not an error, how often does the judge correctly say â€œpassâ€?You want both above 80%. Ideally above 90%.If TPR is low, your judge is missing real problems. If TNR is low, your judge is crying wolf constantly.Both need to be high.Iterate on Your PromptYour first judge prompt wonâ€™t be perfect. Thatâ€™s expected.Look at the cases where the judge got it wrong:Where did it miss real failures? Add those scenarios to your criteria.Where did it false alarm? Add those to your â€œNOT a failureâ€ section.Add 1-2 examples to the prompt showing correct judgments.Test again. Keep iterating until both metrics are above 80%.This takes time. Budget for it. But once you have a reliable judge, you can scale eval across all production traces.5. When to Use Workflows vs Agents vs CodeNot everything needs an LLM judge. Sometimes code-based evals work better.Understanding when to use AI agents versus traditional workflows is critical for building reliable AI products.Hereâ€™s when to use each approach.Code-Based EvalsUse code when you can test something without calling an LLM:Example 1: Format validationIs markdown appearing in text messages? Write a regex that checks for asterisks, square brackets, or hash symbols.No LLM needed. Fast, cheap, deterministic.Example 2: Tool call validationDid the AI call the right tool? Check the tool name in the trace.Did it include required parameters? Check the parameter object.Again, no LLM needed.When to use code evals Format issues, required field checks, tool selection logic, response length constraints, prohibited content patterns.LLM-Based EvalsUse LLMs when you need subjective judgment:Example 1: Human handoff appropriatenessShould this situation have been handed to a human? That requires understanding context and applying policy. An LLM can do this.Example 2: Response qualityIs the response helpful and professional? Code canâ€™t judge that. An LLM can.Example 3: Fact-checking against source materialDid the AI accurately summarize the retrieved documents? An LLM can compare.When to use LLM evalsSubjective quality, policy application, context understanding, summarization accuracy, tone appropriateness.The Decision RuleIf you can express the check as an if/else statement, use code. If you need judgment, use an LLM.Code evals are faster and cheaper. Use them whenever possible. Use LLM evals for everything else.Both are valuable. A complete eval suite typically has 2-3 code-based evals and 1-2 LLM-based evals.6. Critical Mistakes Teams MakeAfter training hundreds of people at top AI companies, Hamel and Shreya see the same mistakes repeatedly.Mistake 1 - Skipping Error AnalysisTeams jump straight to building LLM judges or dashboards. They want the cool technical solution.But they donâ€™t know what theyâ€™re measuring. So they build judges for generic things like â€œhelpfulnessâ€ or â€œconcisenessâ€ that donâ€™t catch real problems.The bathroom being connected instead of disconnected? That â€œhelpfulâ€ judge gave it a pass.The markdown rendering in text messages? That â€œconciseâ€ judge thought it was great.Error analysis is not optional. Itâ€™s the foundation. Skip it and everything else is built on sand.Mistake 2 - Separating the Prompt from the PMThe system prompt is written in English. Itâ€™s made for domain experts.Yet many companies treat it as engineering territory. The PM writes a PRD. The engineer writes the prompt. The engineer has no idea if the product is good.Thatâ€™s backwards. This is a critical part of AI product strategy.PMs should own the prompt. Or at minimum, be deeply involved in prompt iteration.Some teams build admin interfaces where PMs can edit prompts directly in production. Thatâ€™s the right approach.The prompt IS the product in AI applications. Donâ€™t delegate product to engineering.Mistake 3 - Using Only Agreement for Judge ValidationWe covered this already but itâ€™s worth repeating.90% agreement sounds great until you realize your judge is useless because itâ€™s just guessing â€œpassâ€ every time.Always measure TPR and TNR separately. Both must be high.Mistake 4 - Not Doing Eval Until After LaunchSome teams think evals are only for production monitoring.Wrong. Do evals during development.Build your eval suite before you ship. Use it to iterate on your prompts and logic. Use it to catch problems before users do.Evals arenâ€™t just for monitoring. Theyâ€™re for building.Mistake 5 - Outsourcing Error AnalysisSome PMs see error analysis and think â€œthis is tedious, Iâ€™ll have an intern do it.â€Terrible idea.Error analysis is where you build product intuition. Itâ€™s where you learn what users actually need. Itâ€™s where you discover product insights.This is not grunt work to delegate. This is PM core work.The teams shipping the best AI products have PMs whoâ€™ve personally reviewed hundreds or thousands of traces.That experience shows in the product.7. Building Your Eval PracticeYouâ€™re convinced. You want to build an eval practice. Where do you start?If youâ€™re taking an AI product from prototype to production, evals are non-negotiable.Step 1 - Instrument Your CodeBefore anything else, you need traces. Add logging to your AI application:Log every user inputLog every LLM call (prompt and response)Log every tool call and resultLog the final outputUse an observability tool or just log to files. Doesnâ€™t matter. Get the data flowing.Budget 1-2 days for this if youâ€™re starting from scratch.Step 2 - Do Error Analysis on 100 TracesReview 100 traces manually. Take notes. Categorize errors. Count them.This is the most important step. Donâ€™t skip it. Donâ€™t rush it.Budget 2-3 hours for the review. Budget another 1-2 hours for categorization and counting.At the end youâ€™ll have a prioritized list of whatâ€™s broken.Step 3 - Fix Obvious IssuesSome issues donâ€™t need evals. They just need fixing.Markdown in text messages? Add â€œnever use markdown formattingâ€ to your prompt.Missing a tool for virtual tours? Add it or document the limitation.Knock out the easy wins. This builds momentum.Step 4 - Build One LLM Judge for Your Biggest IssuePick your most important error category. Build an LLM judge for it.Write the prompt. Test it on your labeled traces. Iterate until TPR and TNR are both above 80%.This first judge will take the longest. Budget a day or two.But youâ€™ll learn the process. Future judges will be faster.Step 5 - Build Code-Based Evals for Systematic IssuesLook for patterns you can check with code. Build those evals.Format validation. Required field checks. Response length limits.These are quick wins that catch whole classes of problems.Step 6 - IterateUse your evals to improve the product. Review traces where evals caught issues. Fix the underlying problems.Then do error analysis again on new traces. Build new evals as needed.This cycle continues forever. Your product evolves. New edge cases emerge. Your eval suite grows with it.The End StateAfter a few months, youâ€™ll have:2-3 code-based evals running on every request1-2 LLM-based evals running regularlyA practice of doing error analysis monthly to catch new issuesConfidence that your AI product actually works in productionThatâ€™s the goal.Get TranscriptWhere To Find ThemLinkedIn:Hamel: Hamelâ€™s LinkedInShreya: Shreyaâ€™s LinkedInAI Evals Course: Worldâ€™s best AI Evals Course (You get $800 off with this link)Related ContentNewsletters:AI EvalsAI PM ObservabilityAI TestingLLM JudgeAI Prototype to ProductionAI Product StrategyHow to Build AI ProductsPrompt EngineeringPodcasts:AI Evals: Everything You Need to Know to StartEverything you need to know about AI (for PMs and builders)Carl Vellotti on Claude CodeMarily Nika on Google AI PM Tool StackPawel Huryn on n8n for PMsPS. Please subscribe on YouTube and follow on Apple & Spotify. It helps!",
  "harvested_at": "2026-01-28T22:49:08.570583"
}